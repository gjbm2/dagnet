version: "1.0.0"
connections:
  # Amplitude - Analytics & Funnel Data
  # Uses Amplitude Dashboard REST API (Funnels)
  # Docs: https://developers.amplitude.com/docs/dashboard-rest-api
  # NOTE: Time window defaults to last 7 calendar days (until Window Selector UI is implemented)
  - name: amplitude-prod
    provider: amplitude
    kind: http
    description: "Production Amplitude analytics for conversion funnel data"
    enabled: true
    credsRef: amplitude
    # NOTE: Demo/test credentials - events may not exist in the test project
    # For real data, configure actual Amplitude project credentials
    defaults:
      # Region-specific base URL:
      # - US: https://amplitude.com/api/2
      # - EU: https://analytics.eu.amplitude.com/api/2
      base_url: "https://amplitude.com/api/2"
      # Cohort exclusions (internal test users - commercially irrelevant)
      # NOTE: Create amplitude-test connection without this exclusion for internal testing
      excluded_cohorts:
        - "9z057h6i"  # Internal test users cohort
    connection_string_schema:
      type: object
      properties:
        segment:  # Optional segmentation/filter definition (passes through to 's')
          type: array
          items:
            type: object
          description: "Optional array of segment filter objects as expected by Amplitude API"
    adapter:
      # NOTE: This pre_request transformation is REQUIRED for Amplitude
      # because DagNet query from(B).to(C).visited(A) must be transformed to
      # a full funnel A>B>C, then extract only the B>C conversion
      pre_request:
        script: |
          // Helper: Convert filter operator to Amplitude format
          const mapOperator = (op) => {
            const mapping = {
              "is": "is",
              "is not": "is not",
              "is any of": "is",
              "is not any of": "is not",
              "contains": "contains",
              "does not contain": "does not contain"
            };
            return mapping[op] || "is";
          };
          
          // Helper: Build event step with optional filters
          const buildEventStep = (eventName, filters) => {
            const step = { event_type: eventName };
            
            // Add property filters if defined
            if (filters && filters.length > 0) {
              step.filters = filters.map(f => ({
                subprop_type: "event",
                subprop_key: f.property,
                subprop_op: mapOperator(f.operator),
                subprop_value: f.values
              }));
            }
            
            return step;
          };
          
          // Build full funnel including visited events
          const events = [];
          const eventFilters = dsl.event_filters || {};
          
          // Add visited events to funnel (if any)
          if (dsl.visited && dsl.visited.length > 0) {
            events.push(...dsl.visited.map(eventName => 
              buildEventStep(eventName, eventFilters[eventName])
            ));
          }
          
          // Add from → to events
          events.push(buildEventStep(dsl.from, eventFilters[dsl.from]));
          events.push(buildEventStep(dsl.to, eventFilters[dsl.to]));
          
          // Convert window to Amplitude date format (YYYYMMDD)
          const formatDate = (iso) => iso.split('T')[0].replace(/-/g, '');
          const startDate = formatDate(window.start);
          const endDate = formatDate(window.end);
          
          // Build segmentation parameter for cohort exclusions AND case filtering
          // Amplitude API expects: s=[{"prop":"...", "op":"...", "values":[...]}]
          const segments = [];
          
          // Add cohort exclusions (if any)
          if (connection.excluded_cohorts && connection.excluded_cohorts.length > 0) {
            segments.push(...connection.excluded_cohorts.map(cohortId => ({
              prop: "userdata_cohort",
              op: "is not",  // "is not" for exclude
              values: [cohortId]
            })));
          }
          
          // Add case filtering (if any)
          // Pattern: case(coffee-promotion:treatment) 
          // → filter on activeGates.experiment_coffee_promotion = true/false based on variant name
          // Variant → bool policy is implemented in DAS core as dasHelpers.resolveVariantToBool
          // Naming convention: DagNet uses hyphens (coffee-promotion), Statsig/Amplitude uses underscores (experiment_coffee_promotion)
          if (dsl.case && Array.isArray(dsl.case)) {
            for (const caseFilter of dsl.case) {
              const case_id = caseFilter.key;
              const variant = caseFilter.value;
              
              // Transform case_id to gate_id: "coffee-promotion" → "experiment_coffee_promotion"
              // Replace hyphens with underscores (Statsig naming convention)
              const gate_id = case_id.replace(/-/g, '_');
              
              const gateValue = (typeof dasHelpers !== "undefined" && dasHelpers && typeof dasHelpers.resolveVariantToBool === "function")
                ? dasHelpers.resolveVariantToBool(variant)
                : true;
              
              // Filter on activeGates.{gate_id} property
              // Note: This assumes your instrumentation sends activeGates.{gate_id} = true/false
              segments.push({
                prop: `activeGates.${gate_id}`,
                op: "is",
                values: [gateValue ? "true" : "false"]
              });
              
              console.log(`[Amplitude Adapter] Added case filter: case_id="${case_id}" → activeGates.${gate_id} = ${gateValue} (variant: ${variant})`);
            }
          }
          
          // Build s parameter if we have any segments
          let segmentParam = '';
          if (segments.length > 0) {
            segmentParam = 's=' + encodeURIComponent(JSON.stringify(segments));
          }
          
          // Build query parameters for GET request
          // Dashboard REST API expects: /funnels?e={...}&e={...}&start=...&end=...&i=1&s={...}
          // i=1 means daily breakdown (required for time-series)
          // If context.mode is 'aggregate', we could omit i=1, but keeping it for consistency
          const intervalParam = (context && context.mode === 'daily') ? 'i=1' : 'i=1';  // Always daily for now
          const eventParams = events.map(e => 'e=' + encodeURIComponent(JSON.stringify(e))).join('&');
          const otherParams = [
            'start=' + startDate,
            'end=' + endDate,
            intervalParam,
            segmentParam
          ].filter(p => p).join('&');  // Filter out empty strings
          
          dsl.query_params = eventParams + '&' + otherParams;
          dsl.from_step_index = events.length - 2;  // Index of from_event in results
          dsl.to_step_index = events.length - 1;    // Index of to_event in results
          
          console.log('[Amplitude Adapter] Built funnel:', {
            events: events,
            query_params: dsl.query_params,
            from_index: dsl.from_step_index,
            to_index: dsl.to_step_index,
            excluded_cohorts: connection.excluded_cohorts || [],
            case_filters: dsl.case || []
          });
          
          return dsl;
      request:
        # Dashboard REST API uses GET with query parameters
        url_template: "{{{connection.base_url}}}/funnels?{{{dsl.query_params}}}"
        method: GET
        headers:
          # Dashboard REST API uses HTTP Basic auth: API Key (username) + Secret Key (password)
          # Provide base64(apiKey:secretKey) in credentials.basic_auth_b64
          # NOTE: Use triple braces {{{ }}} to prevent HTML escaping of base64 padding (=)
          Authorization: "Basic {{{credentials.basic_auth_b64}}}"
      response:
        extract:
          # Extract aggregate data (cumulativeRaw) - always available
          - name: from_count
            jmes: "data[0].cumulativeRaw[{{from_step_index}}]"
          - name: to_count
            jmes: "data[0].cumulativeRaw[{{to_step_index}}]"
          # Extract daily time-series data (dayFunnels) - available when i=1
          - name: day_funnels
            jmes: "data[0].dayFunnels"
      transform:
        # Check if we have daily data (context.mode === 'daily')
        - name: mode
          jsonata: "$context.mode ? $context.mode : 'aggregate'"
        - name: time_series
          jsonata: |
            (
              $.mode = 'daily' and $.day_funnels and $.day_funnels.series ?
                $.day_funnels.series ~> $map(function($dayData, $i) {
                  {
                    "date": $.day_funnels.xValues[$i],
                    "n": $dayData[$number($dsl.from_step_index)],
                    "k": $dayData[$number($dsl.to_step_index)],
                    "p": $dayData[$number($dsl.to_step_index)] / $dayData[$number($dsl.from_step_index)]
                  }
                }) : []
            )
        - name: p_mean
          jsonata: "$number(from_count) > 0 ? $number(to_count) / $number(from_count) : 0"
        - name: n
          jsonata: "$number(from_count)"
        - name: k
          jsonata: "$number(to_count)"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

  # Amplitude Test - Include internal test users (disabled by default)
  # Uncomment and enable this connection to analyze internal test user behavior
  # - name: amplitude-test
  #   provider: amplitude
  #   kind: http
  #   description: "Amplitude analytics INCLUDING internal test users"
  #   enabled: false
  #   credsRef: amplitude
  #   defaults:
  #     base_url: "https://amplitude.com/api/2"
  #     # NO excluded_cohorts - includes all users including test cohort 9z057h6i
  #   connection_string_schema:
  #     type: object
  #     properties:
  #       segment:
  #         type: array
  #         items:
  #           type: object
  #   adapter:
  #     # Use same adapter configuration as amplitude-prod
  #     # (copy pre_request, request, response, transform, upsert sections if needed)

  # Google Sheets - Parameter Data
  - name: sheets-readonly
    provider: google-sheets
    kind: http
    auth_type: google-service-account  # Tells DAS Runner to auto-generate OAuth tokens
    description: "Read-only access to Google Sheets for parameter data"
    enabled: true
    credsRef: google-sheets
    requires_event_ids: false  # Google Sheets reads from spreadsheet ranges, doesn't need node event_ids
    defaults:
      api_version: "v4"
    connection_string_schema:
      type: object
      required: [sheet_url]
      properties:
        sheet_url:
          type: string
          description: >
            Google Sheets range URL. Right-click a range in Google Sheets and choose "Copy link to range".
            Example: https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit?gid=0#gid=0&range=A1:B1
        mode:
          type: string
          enum: [auto, single, param-pack]
          default: auto
          description: >
            Parse mode: auto-detect, single-cell scalar value, or param pack (JSON/YAML or name/value pairs).
    adapter:
      pre_request:
        script: |
          // Parse Google Sheets URL to extract spreadsheet_id and range
          // URL format: https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/edit?gid={GID}#gid={GID}&range={RANGE}
          // Example: https://docs.google.com/spreadsheets/d/1_ePUOzMF8czrunNW8YSNhdcqzVAt0nTLTKOsY1HYrWw/edit?gid=0#gid=0&range=A1:B1
          
          const sheetUrl = connection_string.sheet_url;
          if (!sheetUrl) {
            throw new Error('sheet_url is required');
          }
          
          // Extract spreadsheet ID from URL path: /d/{SPREADSHEET_ID}/
          const spreadsheetIdMatch = sheetUrl.match(/\/d\/([a-zA-Z0-9_-]+)\//);
          if (!spreadsheetIdMatch) {
            throw new Error('Invalid Google Sheets URL: could not extract spreadsheet ID');
          }
          const spreadsheet_id = spreadsheetIdMatch[1];
          
          // Extract range from URL hash fragment: #gid={GID}&range={RANGE} or #range={RANGE}
          let range = null;
          const hashMatch = sheetUrl.match(/#[^#]*range=([^&]+)/);
          if (hashMatch) {
            range = decodeURIComponent(hashMatch[1]);
          } else {
            // Fallback: try to extract from query string
            const queryMatch = sheetUrl.match(/[?&]range=([^&#]+)/);
            if (queryMatch) {
              range = decodeURIComponent(queryMatch[1]);
            }
          }
          
          if (!range) {
            throw new Error('Invalid Google Sheets URL: could not extract range. Make sure to copy the link with a range selected.');
          }
          
          // Store parsed values in dsl for use in URL template
          dsl.spreadsheet_id = spreadsheet_id;
          dsl.range = range;
          
          return dsl;
      request:
        url_template: "https://sheets.googleapis.com/v4/spreadsheets/{{{dsl.spreadsheet_id}}}/values/{{{dsl.range}}}"
        method: GET
        headers:
          Authorization: "Bearer {{credentials.access_token}}"
      response:
        extract:
          - name: values
            jmes: "values"
      transform:
        - name: parsed_result
          jsonata: |
            (
              dasHelpers.parseSheetsRange ?
                dasHelpers.parseSheetsRange(values) :
                $parseSheetsRange(values)
            )
        - name: scalar_value
          jsonata: "parsed_result.scalarValue"
        - name: param_pack
          jsonata: "parsed_result.paramPack"
        - name: errors
          jsonata: "parsed_result.errors"
      upsert:
        mode: replace
        writes: []  # Google Sheets data is extracted in dataOperationsService.ts, not via upsert

  # Statsig - Feature Gate/Experiment Configuration
  - name: statsig-prod
    provider: statsig
    kind: http
    description: "Production Statsig for experiment variant allocations"
    enabled: true
    credsRef: statsig
    defaults:
      base_url: "https://statsigapi.net/console/v1"
      environment: production
    connection_string_schema:
      type: object
      properties:
        environment:
          type: string
          description: "Statsig environment (overrides default)"
          default: "production"
    adapter:
      pre_request:
        script: |
          // Transform case_id to gate_id: "coffee-promotion" → "experiment_coffee_promotion"
          // DagNet uses hyphens, Statsig uses underscores
          if (typeof caseId === 'string') {
            dsl.gate_id = caseId.replace(/-/g, '_');
            console.log(`[Statsig Adapter] Transformed case_id="${caseId}" → gate_id="${dsl.gate_id}"`);
          } else {
            dsl.gate_id = caseId;
          }
          
          // Get target environment (from connection_string or defaults)
          dsl.target_env = connection_string.environment || connection.environment || 'production';
          console.log(`[Statsig Adapter] Target environment: ${dsl.target_env}`);
          
          return dsl;
      request:
        url_template: "{{{connection.base_url}}}/gates/{{{dsl.gate_id}}}"
        method: GET
        headers:
          STATSIG-API-KEY: "{{credentials.console_api_key}}"
          Content-Type: "application/json"
      response:
        extract:
          - name: gate_id
            jmes: "data.id"
          - name: gate_name
            jmes: "data.name"
          - name: is_enabled
            jmes: "data.isEnabled"
          - name: rules
            jmes: "data.rules"
      transform:
        # Apply heuristic to extract production pass rate
        # Heuristic (from battle-tested analysis):
        # 1. Filter rules by target environment (e.g., "production" in environments array)
        # 2. Prioritize "public" rules (type=public)
        # 3. If multiple rules, use first public rule's passPercentage
        # 4. Return passPercentage as decimal (0.0 to 1.0)
        - name: target_env
          jsonata: "dsl.target_env"
        - name: production_rules
          jsonata: |
            (
              $target := $exists(target_env) ? target_env : 'production';
              
              /* Filter rules that apply to target environment */
              $filtered := rules[
                $exists(environments) ? 
                  ($target in environments) : 
                  false
              ];
              
              /* Log filtered rules */
              $count($filtered) > 0 ? 
                $filtered : 
                []
            )
        - name: pass_percentage_decimal
          jsonata: |
            (
              /* Find first 'public' rule in production_rules */
              $publicRule := production_rules[conditions[0].type = 'public'][0];
              
              $exists($publicRule) ? 
                ($publicRule.passPercentage / 100) : 
                0.0
            )
        # Compute variant weights
        # DagNet model: case has 2 variants (treatment, control)
        # treatment gets pass%, control gets (1 - pass%)
        - name: treatment_weight
          jsonata: "pass_percentage_decimal"
        - name: control_weight
          jsonata: "1 - pass_percentage_decimal"
        # Build variant array for upsert
        # Note: We assume case has variants named 'treatment' and 'control'
        # In the future, this could be made configurable
        - name: variants_update
          jsonata: |
            [
              {
                "name": "treatment",
                "weight": treatment_weight
              },
              {
                "name": "control",
                "weight": control_weight
              }
            ]
      upsert:
        mode: replace
        writes:
          # Write to case file (using caseId, not nodeId)
          - target: "/case-{{caseId}}/schedules/-"
            value: |
              {
                "window_from": "{{window.start}}",
                "window_to": null,
                "variants": {{{variants_update}}}
              }

  # PostgreSQL - Example for SQL-based data sources (disabled by default)
  - name: postgres-analytics
    provider: postgres
    kind: sql
    description: "PostgreSQL analytics database (example - configure credentials to enable)"
    enabled: false
    credsRef: postgres
    defaults:
      schema: public
    connection_string_schema:
      type: object
      properties:
        table:
          type: string
          description: "Table name for query"
    adapter:
      request:
        url_template: "{{{credentials.host}}}:{{{credentials.port}}}/{{{credentials.database}}}"
        method: POST
        headers:
          Content-Type: "application/json"
        body_template: |
          {
            "query": "SELECT COUNT(*) as n, SUM(CASE WHEN converted = true THEN 1 ELSE 0 END) as k FROM {{connection.schema}}.{{connection_string.table}} WHERE event_from = '{{from_event_id}}' AND event_to = '{{to_event_id}}' AND timestamp BETWEEN '{{window.start}}' AND '{{window.end}}'"
          }
      response:
        extract:
          - name: n
            jmes: "rows[0].n"
          - name: k
            jmes: "rows[0].k"
      transform:
        - name: p_mean
          jsonata: "k / n"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

