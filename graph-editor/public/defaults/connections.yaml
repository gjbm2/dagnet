version: "1.0.0"
connections:
  # Amplitude - Analytics & Funnel Data
  # Uses Amplitude Dashboard REST API (Funnels)
  # Docs: https://developers.amplitude.com/docs/dashboard-rest-api
  # NOTE: Time window defaults to last 7 calendar days (until Window Selector UI is implemented)
  - name: amplitude-prod
    provider: amplitude
    kind: http
    description: "Production Amplitude analytics for conversion funnel data"
    enabled: true
    credsRef: amplitude
    # Provider capabilities for MSMDC query generation
    capabilities:
      supports_native_exclude: true  # Native segment filters support exclude() via inline behavioral cohorts
      supports_visited: true
      supports_ordered: true
      max_funnel_length: 10
      supports_daily_time_series: true  # Amplitude supports per-day data retrieval for incremental fetch
    # NOTE: Demo/test credentials - events may not exist in the test project
    # For real data, configure actual Amplitude project credentials
    defaults:
      # Region-specific base URL:
      # - US: https://amplitude.com/api/2
      # - EU: https://analytics.eu.amplitude.com/api/2
      base_url: "https://amplitude.com/api/2"
      # Cohort exclusions (internal test users - commercially irrelevant)
      # NOTE: Create amplitude-test connection without this exclusion for internal testing
      excluded_cohorts:
        - "9z057h6i"  # Internal test users cohort
    connection_string_schema:
      type: object
      properties:
        segment:  # Optional segmentation/filter definition (passes through to 's')
          type: array
          items:
            type: object
          description: "Optional array of segment filter objects as expected by Amplitude API"
    adapter:
      # NOTE: This pre_request transformation is REQUIRED for Amplitude
      # because DagNet query from(B).to(C).visited(A) must be transformed to
      # a full funnel A>B>C, then extract only the B>C conversion
      pre_request:
        script: |
          // Helper: Convert filter operator to Amplitude format
          const mapOperator = (op) => {
            const mapping = {
              "is": "is",
              "is not": "is not",
              "is any of": "is",
              "is not any of": "is not",
              "contains": "contains",
              "does not contain": "does not contain"
            };
            return mapping[op] || "is";
          };
          
          // Helper: Get event info from eventDefinitions
          // Returns { providerName, filters } for an event_id
          const getEventInfo = (eventId) => {
            const eventDef = eventDefinitions[eventId];
            if (!eventDef) {
              console.warn(`[Amplitude Adapter] No event definition for "${eventId}", using as-is`);
              return { providerName: eventId, filters: [] };
            }
            const providerName = eventDef.provider_event_names?.amplitude || eventId;
            const filters = eventDef.amplitude_filters || [];
            return { providerName, filters };
          };
          
          // Helper: Build event step from event_id
          const buildEventStepFromId = (eventId) => {
            const { providerName, filters } = getEventInfo(eventId);
            const step = { event_type: providerName };
            
            // Add property filters if defined
            if (filters && filters.length > 0) {
              step.filters = filters.map(f => ({
                subprop_type: "event",
                subprop_key: f.property,
                subprop_op: mapOperator(f.operator),
                subprop_value: f.values
              }));
            }
            
            return step;
          };
          
          // Build full funnel including visited events
          const events = [];
          
          // Build segmentation parameter for cohort exclusions AND case filtering
          // Amplitude API expects: s=[{"prop":"...", "op":"...", "values":[...]}]
          // NOTE: We build segments FIRST so we can clear visited_upstream/excludes
          // before the funnel building code runs
          const segments = [];
          
          // Add cohort exclusions (if any) - controlled by excludeTestAccounts setting
          // When context.excludeTestAccounts is false, skip adding the cohort exclusions
          const shouldExcludeTestAccounts = context && context.excludeTestAccounts !== false;  // Default to true
          if (shouldExcludeTestAccounts && connection.excluded_cohorts && connection.excluded_cohorts.length > 0) {
            console.log('[Amplitude Adapter] Excluding test account cohorts:', connection.excluded_cohorts);
            segments.push(...connection.excluded_cohorts.map(cohortId => ({
              prop: "userdata_cohort",
              op: "is not",  // "is not" for exclude
              values: [cohortId]
            })));
          } else if (!shouldExcludeTestAccounts) {
            console.log('[Amplitude Adapter] Test account exclusion DISABLED - including all users');
          }
          
          // ═══════════════════════════════════════════════════════════════════════
          // NATIVE SEGMENT FILTERS for visited() and minus()/exclude()
          // Discovered 4-Dec-25: Amplitude supports inline behavioral cohorts in segments
          // This replaces the super-funnel approach with simpler, more accurate segment filters
          // CRITICAL: This MUST run BEFORE funnel building to clear visited_upstream
          // ═══════════════════════════════════════════════════════════════════════
          
          // Native visited() support - add segment filters for upstream visited nodes
          if (queryPayload.visited_upstream && queryPayload.visited_upstream.length > 0) {
            console.log('[Amplitude Adapter] Using NATIVE segment filters for visited_upstream:', queryPayload.visited_upstream);
            for (const eventId of queryPayload.visited_upstream) {
              const eventDef = buildEventStepFromId(eventId);
              segments.push({
                type: "event",
                event_type: eventDef.event_type,
                filters: eventDef.filters || [],
                op: ">=",
                value: 1,
                time_type: "rolling",
                time_value: 366  // Look back ~1 year
              });
            }
            // Clear visited_upstream so we don't ALSO build a super-funnel (DEPRECATED)
            queryPayload.visited_upstream = [];
          }
          
          // Native minus()/exclude() support - add segment filters for excluded nodes
          if (queryPayload.excludes && queryPayload.excludes.length > 0) {
            console.log('[Amplitude Adapter] Using NATIVE segment filters for excludes:', queryPayload.excludes);
            for (const eventId of queryPayload.excludes) {
              const eventDef = buildEventStepFromId(eventId);
              segments.push({
                type: "event",
                event_type: eventDef.event_type,
                filters: eventDef.filters || [],
                op: "=",
                value: 0,
                time_type: "rolling",
                time_value: 366
              });
            }
            // Clear excludes so downstream code doesn't try composite query handling (DEPRECATED)
            queryPayload.excludes = [];
          }
          
          // Legacy exclude() field (if used) - now supported via native segment filters
          if (queryPayload.exclude && queryPayload.exclude.length > 0) {
            console.log('[Amplitude Adapter] Using NATIVE segment filters for exclude:', queryPayload.exclude);
            for (const eventId of queryPayload.exclude) {
              const eventDef = buildEventStepFromId(eventId);
              segments.push({
                type: "event",
                event_type: eventDef.event_type,
                filters: eventDef.filters || [],
                op: "=",
                value: 0,
                time_type: "rolling",
                time_value: 366
              });
            }
            queryPayload.exclude = [];
          }
          
          // Build funnel with proper ordering:
          // ═══════════════════════════════════════════════════════════════════════
          // DEPRECATED: Super-funnel approach (4-Dec-25)
          // 
          // The super-funnel approach prepended upstream visited nodes to the funnel:
          //   visited_upstream → from → visited (between) → to
          // 
          // This is now BYPASSED because visited_upstream is cleared by native segment
          // filters ABOVE. The code remains for non-Amplitude providers or fallback.
          // 
          // Target deletion: After 2 weeks of production validation.
          // ═══════════════════════════════════════════════════════════════════════
          
          // Track the index where 'from' will be placed (for n/k extraction)
          let fromStepIndex = 0;
          
          // 1. Add upstream visited events FIRST (events that must happen BEFORE 'from')
          // NOTE: This will NOT execute for Amplitude - visited_upstream is cleared by segment filters ABOVE
          // queryPayload.visited_upstream contains event_ids
          if (queryPayload.visited_upstream && queryPayload.visited_upstream.length > 0) {
            console.log('[Amplitude Adapter] DEPRECATED: Building SUPER-FUNNEL with upstream visited nodes:', queryPayload.visited_upstream);
            events.push(...queryPayload.visited_upstream.map(eventId => buildEventStepFromId(eventId)));
            fromStepIndex = queryPayload.visited_upstream.length;
          }
          
          // 2. Add 'from' event (queryPayload.from is the event_id)
          events.push(buildEventStepFromId(queryPayload.from));
          
          // 3. Add visited events between from and to (middle steps)
          // queryPayload.visited contains event_ids
          if (queryPayload.visited && queryPayload.visited.length > 0) {
            events.push(...queryPayload.visited.map(eventId => buildEventStepFromId(eventId)));
          }
          
          // 4. End with 'to' (queryPayload.to is the event_id)
          events.push(buildEventStepFromId(queryPayload.to));
          
          // ═══════════════════════════════════════════════════════════════════════
          // COHORT MODE: For latency-tracked edges, use A-anchored cohort dates
          // If cohort is provided (from parsed cohort() DSL), we:
          // 1. Prepend anchor node as step 0 (if anchor_event_id provided)
          // 2. Use cohort entry dates (cohort.start/end) instead of event dates
          // 3. Set conversion window (cs) based on conversion_window_days
          // ═══════════════════════════════════════════════════════════════════════
          let isCohortMode = false;
          let cohortAnchorStepAdded = false;
          
          if (cohort && (cohort.start || cohort.end)) {
            isCohortMode = true;
            console.log('[Amplitude Adapter] COHORT MODE enabled:', cohort);
            
            // If anchor_event_id is provided and different from 'from', prepend it
            if (cohort.anchor_event_id && cohort.anchor_event_id !== queryPayload.from) {
              // Shift existing step indices
              fromStepIndex = fromStepIndex + 1;
              queryPayload.to_step_index = events.length;  // Will become events.length after unshift
              
              // Prepend anchor event as first step
              events.unshift(buildEventStepFromId(cohort.anchor_event_id));
              cohortAnchorStepAdded = true;
              
              console.log('[Amplitude Adapter] Added anchor step at index 0:', cohort.anchor_event_id);
              console.log('[Amplitude Adapter] Adjusted indices: from=' + fromStepIndex + ', to=' + (events.length - 1));
            }
          }
          
          // Convert window/cohort dates to Amplitude date format (YYYYMMDD)
          // ------------------------------------------------------------------
          // INPUT CONTRACT (Regression Guard)
          //
          // DAGNet internally uses UK dates (`d-MMM-yy`) for UI/logging, but the
          // DAS adapter boundary MUST receive ISO timestamps.
          //
          // If we accidentally pass "6-Dec-25" into DAS, this adapter will
          // produce "6Dec25" which Amplitude rejects (400).
          // ------------------------------------------------------------------
          const assertIsoDateTime = (value, label) => {
            if (typeof value !== 'string' || !/^\d{4}-\d{2}-\d{2}T/.test(value)) {
              throw new Error(`[Amplitude Adapter] ${label} must be ISO (YYYY-MM-DDTHH:mm:ss.sssZ). Got: ${String(value)}`);
            }
          };

          const formatDate = (iso) => iso.split('T')[0].replace(/-/g, '');
          
          // Use cohort dates if in cohort mode, otherwise use window dates
          let startDate, endDate;
          if (isCohortMode && cohort.start && cohort.end) {
            assertIsoDateTime(cohort.start, 'cohort.start');
            assertIsoDateTime(cohort.end, 'cohort.end');
            // Cohort mode: dates are A-entry dates
            startDate = formatDate(cohort.start);
            endDate = formatDate(cohort.end);
            console.log('[Amplitude Adapter] Using COHORT dates:', { start: startDate, end: endDate });
          } else {
            assertIsoDateTime(window.start, 'window.start');
            assertIsoDateTime(window.end, 'window.end');
            // Window mode: dates are X-event dates (CRITICAL: Use window from execution context!)
            startDate = formatDate(window.start);
            endDate = formatDate(window.end);
            console.log('[Amplitude Adapter] Using WINDOW dates:', { start: startDate, end: endDate });
          }
          
          // Add case filtering (if any)
          // Pattern: case(coffee-promotion:treatment) 
          // → filter on activeGates.experiment_coffee_promotion = true/false based on variant name
          // Variant → bool policy is implemented in DAS core as dasHelpers.resolveVariantToBool
          // Naming convention: DagNet uses hyphens (coffee-promotion), Statsig/Amplitude uses underscores (experiment_coffee_promotion)
          if (queryPayload.case && Array.isArray(queryPayload.case)) {
            for (const caseFilter of queryPayload.case) {
              const case_id = caseFilter.key;
              const variant = caseFilter.value;
              
              // Transform case_id to gate_id: "coffee-promotion" → "experiment_coffee_promotion"
              // Replace hyphens with underscores (Statsig naming convention)
              const gate_id = case_id.replace(/-/g, '_');
              
              const gateValue = (typeof dasHelpers !== "undefined" && dasHelpers && typeof dasHelpers.resolveVariantToBool === "function")
                ? dasHelpers.resolveVariantToBool(variant)
                : true;
              
              // Filter on activeGates.{gate_id} property
              // Note: This assumes your instrumentation sends activeGates.{gate_id} = true/false
              segments.push({
                prop: `activeGates.${gate_id}`,
                op: "is",
                values: [gateValue ? "true" : "false"]
              });
              
              console.log(`[Amplitude Adapter] Added case filter: case_id="${case_id}" → activeGates.${gate_id} = ${gateValue} (variant: ${variant})`);
            }
          }
          
          // Add context filtering (if any) as SEGMENT filters
          // IMPORTANT: Segment filters operate on USER PROPERTIES in this API.
          // For custom user properties, Amplitude expects "gp:<name>" as the prop key.
          //
          // Context filters are now STRUCTURED OBJECTS from buildDslFromEdge:
          //   { field: "utm_medium", op: "is", values: ["cpc"] }
          //   { field: "utm_medium", op: "is", values: ["Paid Social", "paidsocial"] }
          //   { field: "utm_medium", op: "is not", values: ["cpc", "referral"] }
          //   { field: "utm_medium", op: "matches", pattern: "paidsocial|Paid Social", patternFlags: "i" }
          //
          // We convert these to Amplitude segment objects:
          //   { prop: "gp:utm_medium", op: "is", values: ["cpc"] }
          //   { prop: "gp:utm_medium", op: "is not", values: ["cpc", "referral"] }
          //   { prop: "gp:utm_medium", op: "contains", values: ["paidsocial"] }  // for regex patterns
          if (queryPayload.context_filters && Array.isArray(queryPayload.context_filters)) {
            const BUILT_IN_USER_PROPS = new Set([
              "version", "country", "city", "region", "DMA", "language",
              "platform", "os", "device", "device_type", "device_family", "start_version", "paying",
              "userdata_cohort"
            ]);
            
            // Helper to normalize property name to Amplitude user property key
            const normalizeProp = (prop) => {
              if (!prop) return prop;
              if (BUILT_IN_USER_PROPS.has(prop)) return prop;
              if (prop.startsWith("gp:")) return prop;
              // Treat everything else as custom user property
              return `gp:${prop}`;
            };
            
            for (const filterObj of queryPayload.context_filters) {
              // Handle both structured objects (new) and string expressions (legacy)
              if (typeof filterObj === 'string') {
                console.warn(`[Amplitude Adapter] Legacy string filter not supported: ${filterObj}`);
                continue;
              }
              
              const prop = normalizeProp(filterObj.field);
              
              if (filterObj.pattern) {
                // Regex pattern - Amplitude doesn't support regex on segments directly
                // Extract literal values from regex patterns like "^(Paid Social|paidsocial)$"
                // Strip regex anchors and extract alternatives from groups
                let patternStr = filterObj.pattern;
                // Remove anchors
                patternStr = patternStr.replace(/^\^/, '').replace(/\$$/, '');
                // Remove outer group parens if present
                if (patternStr.startsWith('(') && patternStr.endsWith(')')) {
                  patternStr = patternStr.slice(1, -1);
                }
                // Now split by | to get literal alternatives
                const patternParts = patternStr.split('|').map(s => s.trim()).filter(s => s.length > 0);
                const op = filterObj.op === 'is not' ? 'is not' : 'is';
                segments.push({
                  prop,
                  op,
                  values: patternParts
                });
                console.log(`[Amplitude Adapter] Added pattern context segment filter: ${prop} ${op} ${JSON.stringify(patternParts)}`);
              } else if (filterObj.values && filterObj.values.length > 0) {
                // Simple values
                const op = filterObj.op === 'is not' ? 'is not' : 'is';
                segments.push({
                  prop,
                  op,
                  values: filterObj.values
                });
                console.log(`[Amplitude Adapter] Added context segment filter: ${prop} ${op} ${JSON.stringify(filterObj.values)}`);
              } else {
                console.warn(`[Amplitude Adapter] Empty filter object: ${JSON.stringify(filterObj)}`);
              }
            }
          }
          
          // Build s parameter if we have any segments
          let segmentParam = '';
          if (segments.length > 0) {
            segmentParam = 's=' + encodeURIComponent(JSON.stringify(segments));
          }
          
          // Build query parameters for GET request
          // Dashboard REST API expects: /funnels?e={...}&e={...}&start=...&end=...&i=1&s={...}&cs=...
          // i=1 means daily breakdown (required for time-series)
          // cs=seconds is conversion window (30 days default for window mode, or conversion_window_days for cohort mode)
          const intervalParam = (context && context.mode === 'daily') ? 'i=1' : 'i=1';  // Always daily for now
          
          // Build conversion window parameter (cs)
          // cs is in seconds - tells Amplitude how long to wait for conversions after entry
          // COHORT mode: use conversion_window_days from edge latency config
          // WINDOW mode: use default 30 days (ensures consistent conversion attribution)
          const DEFAULT_WINDOW_CONVERSION_DAYS = 30;
          let csParam = '';
          let csDays = DEFAULT_WINDOW_CONVERSION_DAYS;  // Default for window mode
          
          if (isCohortMode && cohort.conversion_window_days && cohort.conversion_window_days > 0) {
            csDays = cohort.conversion_window_days;
            console.log('[Amplitude Adapter] COHORT conversion window: ' + csDays + ' days');
          } else {
            console.log('[Amplitude Adapter] WINDOW conversion window (default): ' + csDays + ' days');
          }
          
          const csSeconds = csDays * 24 * 60 * 60;  // Convert days to seconds
          csParam = 'cs=' + csSeconds;
          
          const eventParams = events.map(e => 'e=' + encodeURIComponent(JSON.stringify(e))).join('&');
          const otherParams = [
            'start=' + startDate,
            'end=' + endDate,
            intervalParam,
            segmentParam,
            csParam  // Conversion window (cohort or default 30 days)
          ].filter(p => p).join('&');  // Filter out empty strings
          
          queryPayload.query_params = eventParams + '&' + otherParams;
          
          // CRITICAL: Set step indices for n/k extraction
          // With super-funnel, 'from' may not be at index 0 if there are upstream visited nodes
          queryPayload.from_step_index = fromStepIndex;  // Index of 'from' step (after any upstream visited)
          queryPayload.to_step_index = events.length - 1;  // 'to' is always last step
          
          return queryPayload;
      request:
        # Dashboard REST API uses GET with query parameters
        url_template: "{{{connection.base_url}}}/funnels?{{{queryPayload.query_params}}}"
        method: GET
        headers:
          # Dashboard REST API uses HTTP Basic auth: API Key (username) + Secret Key (password)
          # Provide base64(apiKey:secretKey) in credentials.basic_auth_b64
          # NOTE: Use triple braces {{{ }}} to prevent HTML escaping of base64 padding (=)
          Authorization: "Basic {{{credentials.basic_auth_b64}}}"
      response:
        extract:
          # Extract aggregate data (cumulativeRaw) - always available
          # from_step_index/to_step_index set by pre_request script based on funnel structure
          - name: from_count
            jmes: "data[0].cumulativeRaw[{{from_step_index}}]"
          - name: to_count
            jmes: "data[0].cumulativeRaw[{{to_step_index}}]"
          # Extract daily time-series data (dayFunnels) - available when i=1
          - name: day_funnels
            jmes: "data[0].dayFunnels"
          # === LAG (Latency) Data Extraction ===
          # These fields are extracted for latency-tracked edges (cohort mode)
          # Per-day transition time arrays (parallel to dayFunnels.series)
          # NOTE: Extract .series to get array format expected by JSONata transform
          - name: day_median_trans_times
            jmes: "data[0].dayMedianTransTimes.series"
          - name: day_avg_trans_times
            jmes: "data[0].dayAvgTransTimes.series"
          # Aggregate transition times per step
          - name: median_trans_times
            jmes: "data[0].medianTransTimes"
          - name: avg_trans_times
            jmes: "data[0].avgTransTimes"
          # Histogram of transition time distribution (for lag CDF fitting)
          - name: step_trans_time_distribution
            jmes: "data[0].stepTransTimeDistribution"
      transform:
        # Check if we have daily data (context.mode === 'daily')
        - name: mode
          jsonata: "$context.mode ? $context.mode : 'aggregate'"
        # Build time_series with per-day n/k/p and latency data
        # NOTE: Amplitude returns -1 for lag when there are no converters on that day.
        # We convert -1 to null to avoid tiny negative numbers in our data.
        #
        # For 3-step A→X→Y funnels (cohort mode with anchor):
        #   - from_step_index = 1 (X), to_step_index = 2 (Y)
        #   - median_lag_days at to_step_index = A→Y transition time (used for X→Y lag fit)
        #   - anchor_median_lag_days at from_step_index = A→X transition time
        #     (used to adjust effective age for downstream completeness calculation)
        #   - anchor_n at step 0 = anchor cohort entry count
        #
        # For 2-step X→Y funnels (no anchor):
        #   - from_step_index = 0, to_step_index = 1
        #   - anchor_* values will be 0/null (first latency edge, no upstream)
        - name: time_series
          jsonata: |
            (
              $data := $;
              $toIdx := $number($queryPayload.to_step_index);
              $fromIdx := $number($queryPayload.from_step_index);
              $dates := $data.day_funnels.xValues;
              $medianLags := $data.day_median_trans_times;
              $avgLags := $data.day_avg_trans_times;
              
              $data.mode = 'daily' and $data.day_funnels.series ?
                $data.day_funnels.series ~> $map(function($dayData, $idx) {(
                  $medianRow := $medianLags[$idx];
                  $avgRow := $avgLags[$idx];
                  $medianMs := $medianRow ? $medianRow[$toIdx] : null;
                  $avgMs := $avgRow ? $avgRow[$toIdx] : null;
                  $anchorMedianMs := $medianRow ? $medianRow[$fromIdx] : null;
                  $anchorAvgMs := $avgRow ? $avgRow[$fromIdx] : null;
                  {
                    "date": $dates[$idx],
                    "n": $dayData[$fromIdx],
                    "k": $dayData[$toIdx],
                    "p": $dayData[$fromIdx] > 0 ? $dayData[$toIdx] / $dayData[$fromIdx] : 0,
                    "median_lag_days": $medianMs != null and $medianMs > 0 ? $medianMs / 86400000 : null,
                    "mean_lag_days": $avgMs != null and $avgMs > 0 ? $avgMs / 86400000 : null,
                    "anchor_n": $dayData[0],
                    "anchor_median_lag_days": $anchorMedianMs != null and $anchorMedianMs > 0 ? $anchorMedianMs / 86400000 : null,
                    "anchor_mean_lag_days": $anchorAvgMs != null and $anchorAvgMs > 0 ? $anchorAvgMs / 86400000 : null
                  }
                )}) : []
            )
        # Handle null values gracefully - return 0 when no data
        - name: n
          jsonata: "from_count != null ? $number(from_count) : 0"
        - name: k
          jsonata: "to_count != null ? $number(to_count) : 0"
        - name: p_mean
          jsonata: "n > 0 ? k / n : 0"
        # === LAG (Latency) Aggregate Transforms ===
        # Extract aggregate latency stats for the X→Y transition (to_step_index)
        # NOTE: Amplitude returns -1 for lag when there are no converters. Convert to null.
        - name: median_lag_days
          jsonata: |
            (
              $ms := median_trans_times[$number($queryPayload.to_step_index)];
              $ms != null and $ms > 0 ? $ms / 86400000 : null
            )
        - name: mean_lag_days
          jsonata: |
            (
              $ms := avg_trans_times[$number($queryPayload.to_step_index)];
              $ms != null and $ms > 0 ? $ms / 86400000 : null
            )
        # === Anchor LAG (A→X transition for downstream edges) ===
        # For 3-step funnels, extract A→X lag at from_step_index
        # Used by completeness calculation to adjust effective cohort ages
        - name: anchor_median_lag_days
          jsonata: |
            (
              $ms := median_trans_times[$number($queryPayload.from_step_index)];
              $ms != null and $ms > 0 ? $ms / 86400000 : null
            )
        - name: anchor_mean_lag_days
          jsonata: |
            (
              $ms := avg_trans_times[$number($queryPayload.from_step_index)];
              $ms != null and $ms > 0 ? $ms / 86400000 : null
            )
        # Extract histogram for the X→Y transition (for lag CDF fitting)
        - name: lag_histogram
          jsonata: "step_trans_time_distribution[$number($queryPayload.to_step_index)]"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

  # Amplitude Test - Include internal test users (disabled by default)
  # Uncomment and enable this connection to analyze internal test user behavior
  # - name: amplitude-test
  #   provider: amplitude
  #   kind: http
  #   description: "Amplitude analytics INCLUDING internal test users"
  #   enabled: false
  #   credsRef: amplitude
  #   defaults:
  #     base_url: "https://amplitude.com/api/2"
  #     # NO excluded_cohorts - includes all users including test cohort 9z057h6i
  #   connection_string_schema:
  #     type: object
  #     properties:
  #       segment:
  #         type: array
  #         items:
  #           type: object
  #   adapter:
  #     # Use same adapter configuration as amplitude-prod
  #     # (copy pre_request, request, response, transform, upsert sections if needed)

  # Google Sheets - Parameter Data
  - name: sheets-readonly
    provider: google-sheets
    kind: http
    auth_type: google-service-account  # Tells DAS Runner to auto-generate OAuth tokens
    description: "Read-only access to Google Sheets for parameter data"
    enabled: true
    credsRef: google-sheets
    requires_event_ids: false  # Google Sheets reads from spreadsheet ranges, doesn't need node event_ids
    capabilities:
      supports_native_exclude: false  # Sheets doesn't process queries (data is static)
      supports_visited: false
      supports_ordered: false
    defaults:
      api_version: "v4"
    connection_string_schema:
      type: object
      required: [sheet_url]
      properties:
        sheet_url:
          type: string
          description: >
            Google Sheets range URL. Right-click a range in Google Sheets and choose "Copy link to range".
            Example: https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit?gid=0#gid=0&range=A1:B1
        mode:
          type: string
          enum: [auto, single, param-pack]
          default: auto
          description: >
            Parse mode: auto-detect, single-cell scalar value, or param pack (JSON/YAML or name/value pairs).
    adapter:
      pre_request:
        script: |
          // Parse Google Sheets URL to extract spreadsheet_id and range
          // URL format: https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/edit?gid={GID}#gid={GID}&range={RANGE}
          // Example: https://docs.google.com/spreadsheets/d/1_ePUOzMF8czrunNW8YSNhdcqzVAt0nTLTKOsY1HYrWw/edit?gid=0#gid=0&range=A1:B1
          
          const sheetUrl = connection_string.sheet_url;
          if (!sheetUrl) {
            throw new Error('sheet_url is required');
          }
          
          // Extract spreadsheet ID from URL path: /d/{SPREADSHEET_ID}/
          const spreadsheetIdMatch = sheetUrl.match(/\/d\/([a-zA-Z0-9_-]+)\//);
          if (!spreadsheetIdMatch) {
            throw new Error('Invalid Google Sheets URL: could not extract spreadsheet ID');
          }
          const spreadsheet_id = spreadsheetIdMatch[1];
          
          // Extract range from URL hash fragment: #gid={GID}&range={RANGE} or #range={RANGE}
          let range = null;
          const hashMatch = sheetUrl.match(/#[^#]*range=([^&]+)/);
          if (hashMatch) {
            range = decodeURIComponent(hashMatch[1]);
          } else {
            // Fallback: try to extract from query string
            const queryMatch = sheetUrl.match(/[?&]range=([^&#]+)/);
            if (queryMatch) {
              range = decodeURIComponent(queryMatch[1]);
            }
          }
          
          if (!range) {
            throw new Error('Invalid Google Sheets URL: could not extract range. Make sure to copy the link with a range selected.');
          }
          
          // Store parsed values in queryPayload for use in URL template
          queryPayload.spreadsheet_id = spreadsheet_id;
          queryPayload.range = range;
          
          return queryPayload;
      request:
        url_template: "https://sheets.googleapis.com/v4/spreadsheets/{{{queryPayload.spreadsheet_id}}}/values/{{{queryPayload.range}}}"
        method: GET
        headers:
          Authorization: "Bearer {{credentials.access_token}}"
      response:
        extract:
          - name: values
            jmes: "values"
      transform:
        - name: parsed_result
          jsonata: |
            (
              dasHelpers.parseSheetsRange ?
                dasHelpers.parseSheetsRange(values) :
                $parseSheetsRange(values)
            )
        - name: scalar_value
          jsonata: "parsed_result.scalarValue"
        - name: param_pack
          jsonata: "parsed_result.paramPack"
        - name: errors
          jsonata: "parsed_result.errors"
      upsert:
        mode: replace
        writes: []  # Google Sheets data is extracted in dataOperationsService.ts, not via upsert

  # Statsig - Feature Gate/Experiment Configuration
  - name: statsig-prod
    provider: statsig
    kind: http
    description: "Production Statsig for experiment variant allocations"
    enabled: true
    credsRef: statsig
    capabilities:
      supports_native_exclude: false  # Statsig returns variant allocations (static)
      supports_visited: false
      supports_ordered: false
    defaults:
      base_url: "https://statsigapi.net/console/v1"
      environment: production
    connection_string_schema:
      type: object
      properties:
        environment:
          type: string
          description: "Statsig environment (overrides default)"
          default: "production"
    adapter:
      pre_request:
        script: |
          // Transform case_id to gate_id: "coffee-promotion" → "experiment_coffee_promotion"
          // DagNet uses hyphens, Statsig uses underscores
          if (typeof caseId === 'string') {
            queryPayload.gate_id = caseId.replace(/-/g, '_');
            console.log(`[Statsig Adapter] Transformed case_id="${caseId}" → gate_id="${queryPayload.gate_id}"`);
          } else {
            queryPayload.gate_id = caseId;
          }
          
          // Get target environment (from connection_string or defaults)
          queryPayload.target_env = connection_string.environment || connection.environment || 'production';
          console.log(`[Statsig Adapter] Target environment: ${queryPayload.target_env}`);
          
          return queryPayload;
      request:
        url_template: "{{{connection.base_url}}}/gates/{{{queryPayload.gate_id}}}"
        method: GET
        headers:
          STATSIG-API-KEY: "{{credentials.console_api_key}}"
          Content-Type: "application/json"
      response:
        extract:
          - name: gate_id
            jmes: "data.id"
          - name: gate_name
            jmes: "data.name"
          - name: is_enabled
            jmes: "data.isEnabled"
          - name: rules
            jmes: "data.rules"
      transform:
        # Apply heuristic to extract production pass rate
        # Heuristic (from battle-tested analysis):
        # 1. Filter rules by target environment (e.g., "production" in environments array)
        # 2. Prioritize "public" rules (type=public)
        # 3. If multiple rules, use first public rule's passPercentage
        # 4. Return passPercentage as decimal (0.0 to 1.0)
        - name: target_env
          jsonata: "queryPayload.target_env"
        - name: production_rules
          jsonata: |
            (
              $target := $exists(target_env) ? target_env : 'production';
              
              /* Filter rules that apply to target environment */
              $filtered := rules[
                $exists(environments) ? 
                  ($target in environments) : 
                  false
              ];
              
              /* Log filtered rules */
              $count($filtered) > 0 ? 
                $filtered : 
                []
            )
        - name: pass_percentage_decimal
          jsonata: |
            (
              /* Find first 'public' rule in production_rules */
              $publicRule := production_rules[conditions[0].type = 'public'][0];
              
              $exists($publicRule) ? 
                ($publicRule.passPercentage / 100) : 
                0.0
            )
        # Compute variant weights
        # DagNet model: case has 2 variants (treatment, control)
        # treatment gets pass%, control gets (1 - pass%)
        - name: treatment_weight
          jsonata: "pass_percentage_decimal"
        - name: control_weight
          jsonata: "1 - pass_percentage_decimal"
        # Build variant array for upsert
        # Note: We assume case has variants named 'treatment' and 'control'
        # In the future, this could be made configurable
        - name: variants_update
          jsonata: |
            [
              {
                "name": "treatment",
                "weight": treatment_weight
              },
              {
                "name": "control",
                "weight": control_weight
              }
            ]
      upsert:
        mode: replace
        writes:
          # Write to case file (using caseId, not nodeId)
          - target: "/case-{{caseId}}/schedules/-"
            value: |
              {
                "window_from": "{{window.start}}",
                "window_to": null,
                "variants": {{{variants_update}}}
              }

  # PostgreSQL - Example for SQL-based data sources (disabled by default)
  - name: postgres-analytics
    provider: postgres
    kind: sql
    description: "PostgreSQL analytics database (example - configure credentials to enable)"
    enabled: false
    credsRef: postgres
    capabilities:
      supports_native_exclude: true  # SQL can use NOT IN / EXCEPT clauses
      supports_visited: true
      supports_ordered: true
    defaults:
      schema: public
    connection_string_schema:
      type: object
      properties:
        table:
          type: string
          description: "Table name for query"
    adapter:
      request:
        url_template: "{{{credentials.host}}}:{{{credentials.port}}}/{{{credentials.database}}}"
        method: POST
        headers:
          Content-Type: "application/json"
        body_template: |
          {
            "query": "SELECT COUNT(*) as n, SUM(CASE WHEN converted = true THEN 1 ELSE 0 END) as k FROM {{connection.schema}}.{{connection_string.table}} WHERE event_from = '{{from_event_id}}' AND event_to = '{{to_event_id}}' AND timestamp BETWEEN '{{window.start}}' AND '{{window.end}}'"
          }
      response:
        extract:
          - name: n
            jmes: "rows[0].n"
          - name: k
            jmes: "rows[0].k"
      transform:
        - name: p_mean
          jsonata: "k / n"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

