version: "1.0.0"
connections:
  # Amplitude - Analytics & Funnel Data
  # Uses Amplitude Dashboard REST API (Funnels)
  # Docs: https://developers.amplitude.com/docs/dashboard-rest-api
  # NOTE: Time window defaults to last 7 calendar days (until Window Selector UI is implemented)
  - name: amplitude-prod
    provider: amplitude
    kind: http
    description: "Production Amplitude analytics for conversion funnel data"
    enabled: true
    credsRef: amplitude
    # NOTE: Demo/test credentials - events may not exist in the test project
    # For real data, configure actual Amplitude project credentials
    defaults:
      # Region-specific base URL:
      # - US: https://amplitude.com/api/2
      # - EU: https://analytics.eu.amplitude.com/api/2
      base_url: "https://amplitude.com/api/2"
      # Cohort exclusions (internal test users - commercially irrelevant)
      # NOTE: Create amplitude-test connection without this exclusion for internal testing
      excluded_cohorts:
        - "9z057h6i"  # Internal test users cohort
    connection_string_schema:
      type: object
      properties:
        segment:  # Optional segmentation/filter definition (passes through to 's')
          type: array
          items:
            type: object
          description: "Optional array of segment filter objects as expected by Amplitude API"
    adapter:
      # NOTE: This pre_request transformation is REQUIRED for Amplitude
      # because DagNet query from(B).to(C).visited(A) must be transformed to
      # a full funnel A>B>C, then extract only the B>C conversion
      pre_request:
        script: |
          // Helper: Convert filter operator to Amplitude format
          const mapOperator = (op) => {
            const mapping = {
              "is": "is",
              "is not": "is not",
              "is any of": "is",
              "is not any of": "is not",
              "contains": "contains",
              "does not contain": "does not contain"
            };
            return mapping[op] || "is";
          };
          
          // Helper: Build event step with optional filters
          const buildEventStep = (eventName, filters) => {
            const step = { event_type: eventName };
            
            // Add property filters if defined
            if (filters && filters.length > 0) {
              step.filters = filters.map(f => ({
                subprop_type: "event",
                subprop_key: f.property,
                subprop_op: mapOperator(f.operator),
                subprop_value: f.values
              }));
            }
            
            return step;
          };
          
          // Build full funnel including visited events
          const events = [];
          const eventFilters = dsl.event_filters || {};
          
          // Add visited events to funnel (if any)
          if (dsl.visited && dsl.visited.length > 0) {
            events.push(...dsl.visited.map(eventName => 
              buildEventStep(eventName, eventFilters[eventName])
            ));
          }
          
          // Add from â†’ to events
          events.push(buildEventStep(dsl.from, eventFilters[dsl.from]));
          events.push(buildEventStep(dsl.to, eventFilters[dsl.to]));
          
          // Convert window to Amplitude date format (YYYYMMDD)
          const formatDate = (iso) => iso.split('T')[0].replace(/-/g, '');
          const startDate = formatDate(window.start);
          const endDate = formatDate(window.end);
          
          // Build segmentation parameter for cohort exclusions
          // Amplitude API expects: s=[{"prop":"userdata_cohort","op":"is not","values":["cohortId"]}]
          let segmentParam = '';
          if (connection.excluded_cohorts && connection.excluded_cohorts.length > 0) {
            const segments = connection.excluded_cohorts.map(cohortId => ({
              prop: "userdata_cohort",
              op: "is not",  // "is not" for exclude
              values: [cohortId]
            }));
            // s parameter should be a JSON array, not an object with a segments property
            segmentParam = 's=' + encodeURIComponent(JSON.stringify(segments));
          }
          
          // Build query parameters for GET request
          // Dashboard REST API expects: /funnels?e={...}&e={...}&start=...&end=...&i=1&s={...}
          // i=1 means daily breakdown (required for time-series)
          // If context.mode is 'aggregate', we could omit i=1, but keeping it for consistency
          const intervalParam = (context && context.mode === 'daily') ? 'i=1' : 'i=1';  // Always daily for now
          const eventParams = events.map(e => 'e=' + encodeURIComponent(JSON.stringify(e))).join('&');
          const otherParams = [
            'start=' + startDate,
            'end=' + endDate,
            intervalParam,
            segmentParam
          ].filter(p => p).join('&');  // Filter out empty strings
          
          dsl.query_params = eventParams + '&' + otherParams;
          dsl.from_step_index = events.length - 2;  // Index of from_event in results
          dsl.to_step_index = events.length - 1;    // Index of to_event in results
          
          console.log('[Amplitude Adapter] Built funnel:', {
            events: events,
            query_params: dsl.query_params,
            from_index: dsl.from_step_index,
            to_index: dsl.to_step_index,
            excluded_cohorts: connection.excluded_cohorts || []
          });
          
          return dsl;
      request:
        # Dashboard REST API uses GET with query parameters
        url_template: "{{{connection.base_url}}}/funnels?{{{dsl.query_params}}}"
        method: GET
        headers:
          # Dashboard REST API uses HTTP Basic auth: API Key (username) + Secret Key (password)
          # Provide base64(apiKey:secretKey) in credentials.basic_auth_b64
          # NOTE: Use triple braces {{{ }}} to prevent HTML escaping of base64 padding (=)
          Authorization: "Basic {{{credentials.basic_auth_b64}}}"
      response:
        extract:
          # Extract aggregate data (cumulativeRaw) - always available
          - name: from_count
            jmes: "data[0].cumulativeRaw[{{from_step_index}}]"
          - name: to_count
            jmes: "data[0].cumulativeRaw[{{to_step_index}}]"
          # Extract daily time-series data (dayFunnels) - available when i=1
          - name: day_funnels
            jmes: "data[0].dayFunnels"
      transform:
        # Check if we have daily data (context.mode === 'daily')
        - name: mode
          jsonata: "$context.mode ? $context.mode : 'aggregate'"
        - name: time_series
          jsonata: |
            (
              $mode = 'daily' and $day_funnels and $day_funnels.series ?
                $day_funnels.series ~> $map(function($dayData, $i) {
                  $fromIdx := $number($dsl.from_step_index);
                  $toIdx := $number($dsl.to_step_index);
                  {
                    "date": $day_funnels.xValues[$i],
                    "n": $dayData[$fromIdx],
                    "k": $dayData[$toIdx],
                    "p": $dayData[$toIdx] / $dayData[$fromIdx]
                  }
                }) : []
            )
        - name: p_mean
          jsonata: "to_count / from_count"
        - name: n
          jsonata: "from_count"
        - name: k
          jsonata: "to_count"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

  # Amplitude Test - Include internal test users (disabled by default)
  # Uncomment and enable this connection to analyze internal test user behavior
  # - name: amplitude-test
  #   provider: amplitude
  #   kind: http
  #   description: "Amplitude analytics INCLUDING internal test users"
  #   enabled: false
  #   credsRef: amplitude
  #   defaults:
  #     base_url: "https://amplitude.com/api/2"
  #     # NO excluded_cohorts - includes all users including test cohort 9z057h6i
  #   connection_string_schema:
  #     type: object
  #     properties:
  #       segment:
  #         type: array
  #         items:
  #           type: object
  #   adapter:
  #     # Use same adapter configuration as amplitude-prod
  #     # (copy pre_request, request, response, transform, upsert sections if needed)

  # Google Sheets - Parameter Data
  - name: sheets-readonly
    provider: google-sheets
    kind: http
    auth_type: google-service-account  # Tells DAS Runner to auto-generate OAuth tokens
    description: "Read-only access to Google Sheets for parameter data"
    enabled: true
    credsRef: google-sheets
    defaults:
      api_version: "v4"
    connection_string_schema:
      type: object
      required: [spreadsheet_id, range]
      properties:
        spreadsheet_id:
          type: string
          description: "Google Sheets spreadsheet ID from URL"
        range:
          type: string
          description: "A1 notation range (e.g., 'Sheet1!A1:B10')"
        expected_format:
          type: string
          description: "Optional hint for data format (e.g., 'p_mean, n, k')"
    adapter:
      request:
        url_template: "https://sheets.googleapis.com/v4/spreadsheets/{{{connection_string.spreadsheet_id}}}/values/{{{connection_string.range}}}"
        method: GET
        headers:
          Authorization: "Bearer {{credentials.access_token}}"
      response:
        extract:
          - name: values
            jmes: "values"
      transform:
        - name: p_mean
          jsonata: "$number(values[0][1])"
        - name: n
          jsonata: "$number(values[1][1])"
        - name: k
          jsonata: "$number(values[2][1])"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

  # Statsig - Feature Gate/Experiment Configuration
  - name: statsig-prod
    provider: statsig
    kind: http
    description: "Production Statsig for experiment variant allocations"
    enabled: true
    credsRef: statsig
    defaults:
      base_url: "https://statsigapi.net/console/v1"
      environment: production
    connection_string_schema:
      type: object
      properties:
        project_id:
          type: string
          description: "Statsig project ID (optional override)"
    adapter:
      request:
        url_template: "{{{connection.base_url}}}/gates/{{{caseId}}}"
        method: GET
        headers:
          STATSIG-API-KEY: "{{credentials.console_api_key}}"
          Content-Type: "application/json"
      response:
        extract:
          - name: gate_id
            jmes: "data.id"
          - name: gate_name
            jmes: "data.name"
          - name: enabled
            jmes: "data.enabled"
          - name: rules
            jmes: "data.rules"
      transform:
        - name: variants
          jsonata: |
            rules[type='experiment'].returnValue{
              'variant_id': $string(id),
              'name': name,
              'allocation': passPercentage / 100
            }
      upsert:
        mode: replace
        writes:
          - target: "/nodes/{{nodeId}}/case/variants"
            value: "{{variants}}"

  # PostgreSQL - Example for SQL-based data sources (disabled by default)
  - name: postgres-analytics
    provider: postgres
    kind: sql
    description: "PostgreSQL analytics database (example - configure credentials to enable)"
    enabled: false
    credsRef: postgres
    defaults:
      schema: public
    connection_string_schema:
      type: object
      properties:
        table:
          type: string
          description: "Table name for query"
    adapter:
      request:
        url_template: "{{{credentials.host}}}:{{{credentials.port}}}/{{{credentials.database}}}"
        method: POST
        headers:
          Content-Type: "application/json"
        body_template: |
          {
            "query": "SELECT COUNT(*) as n, SUM(CASE WHEN converted = true THEN 1 ELSE 0 END) as k FROM {{connection.schema}}.{{connection_string.table}} WHERE event_from = '{{from_event_id}}' AND event_to = '{{to_event_id}}' AND timestamp BETWEEN '{{window.start}}' AND '{{window.end}}'"
          }
      response:
        extract:
          - name: n
            jmes: "rows[0].n"
          - name: k
            jmes: "rows[0].k"
      transform:
        - name: p_mean
          jsonata: "k / n"
      upsert:
        mode: replace
        writes:
          - target: "/edges/{{edgeId}}/p/mean"
            value: "{{p_mean}}"
          - target: "/edges/{{edgeId}}/p/evidence/n"
            value: "{{n}}"
          - target: "/edges/{{edgeId}}/p/evidence/k"
            value: "{{k}}"

